[
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/news/helix",
    "key_url": "https://figure.ai/news/helix",
    "page_type": "product",
    "content_hash": "0c706078665a2438a4825da8e0219f1b063bcc50804a73264eeac2bf315ec527",
    "normalized_text_len": 14003,
    "extracted_text_preview": "Introducing Helix\nWe're introducing Helix, a generalist Vision-Language-Action (VLA) model that unifies perception, language understanding, and learned control to overcome multiple longstanding challenges in robotics. Helix is a series of firsts:\nFull-upper-body control: Helix is the first VLA to output high-rate continuous control of the entire humanoid upper body, including wrists, torso, head, and individual fingers.\nMulti-robot collaboration: Helix is the first VLA to operate simultaneously on two robots, enabling them to solve a shared, long-horizon manipulation task with items they have never seen before.\nPick up anything: Figure robots equipped with Helix can now pick up virtually any small household object, including thousands of items they have never encountered before, simply by following natural language prompts.\nOne neural network: Unlike prior approaches, Helix uses a single set of neural network weights to learn all behaviors—picking and placing items, using drawers and refrigerators, and cross-robot interaction—without any task-specific fine-tuning.\nCommercial-ready: Helix is the first VLA that runs entirely onboard embedded low-power-consumption GPUs, making it immediately ready for commercial deployment.\nNew Scaling for Humanoid Robotics\nThe home presents robotics' greatest challenge. Unlike controlled industrial settings, homes are filled with countless objects–delicate glassware, crumpled clothing, scattered toys–each with unpredictable shapes, sizes, colors, and textures. For robots to be useful in households, they will need to be capable of generating intelligent new behaviors on-demand, especially for objects they've never seen before.\nThe current state of robotics will not scale to the home without a step change. Teaching robots even a single new behavior currently requires substantial human effort: either hours of PhD-level expert manual programming or thousands of demonstrations. Both are prohibitively expensive when we consider how vast the problem of the home truly is.\nBut other domains of AI have mastered this kind of instant generalization. What if we could simply translate the rich semantic knowledge captured in Vision Language Models (VLMs) directly into robot actions? This new capability would fundamentally alter robotics' scaling trajectory (Figure 1). Suddenly, new skills that once took hundreds of demonstrations could be obtained instantly just by talking to robots in natural language. The key problem becomes: how do we extract all this common-sense knowledge from VLMs and translate it into generalizable robot control? We built Helix to bridge this gap.\nHelix: A \"System 1, System 2\" VLA for Whole Upper Body Control\nHelix is a first-of-its-kind \"System 1, System 2\" VLA model for high-rate, dexterous control of the entire humanoid upper body.\nPrior approaches face a fundamental tradeoff: VLM backbones are general, but not fast, and robot visuomotor policies are fast but not general. Helix resolves this tradeoff through two complementary systems, trained end-to-end to communicate:\nSystem 2 (S2): An onboard internet-pretrained VLM operating at 7-9 Hz for scene understanding and language comprehension, enabling broad generalization across objects and contexts.\nSystem 1 (S1): A fast reactive visuomotor policy that translates the latent semantic representations produced by S2 into precise continuous robot actions at 200 Hz.\nThis decoupled architecture allows each system to operate at its optimal timescale. S2 can \"think slow\" about high-level goals, while S1 can \"think fast\" to execute and adjust actions in real-time. For example, during collaborative behavior (see Video 2), S1 quickly adapts to the changing motions of a partner robot while maintaining S2's semantic objectives.\nHelix's design offers several key advantages over existing approaches:\nSpeed and Generalization: Helix matches the speed of specialized single-task behavioral cloning policies while generalizing zero-shot to thousands of novel test objects.\nScalability: Helix directly outputs continuous control for high-dimensional action spaces, avoiding complex action tokenization schemes used in prior VLA approaches, which have shown some success in low-dimensional control setups (e.g. binarized parallel grippers) but face scaling challenges with high-dimensional humanoid control.\nArchitectural Simplicity: Helix uses standard architectures - an open source, open weight VLM for System 2 and a simple transformer-based visuomotor policy for S1.\nSeparation of concerns: Decoupling S1 and S2 allows us to iterate on each system separately, without constraints of finding unified observation space or action representations.\nModel and Training Details\nData\nWe collect a high quality, multi-robot, multi-operator dataset of diverse teleoperated behaviors, ~500 hours in total. To generate natural language-conditioned training pairs, we use an auto-labeling VLM to generate hindsight instructions. The VLM processes segmented video clip",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 86143,
    "processed_at": "2025-09-10T11:41:35.497791"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/news/scaling-helix-logistics",
    "key_url": "https://figure.ai/news/scaling-helix-logistics",
    "page_type": "product",
    "content_hash": "551afe9a55286db691c5fe8e10c6dc018ed3578e19d3c480097d58283eb0fff8",
    "normalized_text_len": 13139,
    "extracted_text_preview": "In just three months since our initial deployment of Helix in a logistics environment, the system’s capabilities and performance have leapt forward. Helix can now handle a wider variety of packaging and is approaching human-level dexterity and speed, bringing us closer to fully autonomous package sorting. This rapid progress underscores the scalability of Helix’s learning-based approach to robotics, translating quickly into real-world application.\nNew package types – Helix now manipulates deformable poly bags and flat envelopes as reliably as rigid boxes, adjusting its grasp and strategy for each form factor and handling objects dynamically.\nFaster throughput – Despite handling much more complex and varied package types, execution speed has also improved to 4.05 seconds per package (down from ~5.0s), achieving ~20% faster handling while maintaining accuracy.\nHigher barcode scanning success – Shipping labels are now correctly oriented for scanning ~95% of the time (up from ~70%), through better vision and control.\nAdaptive behaviors – The robot exhibits subtle behaviors learned from demonstration, such as gently patting down plastic mailers to flatten wrinkles and improve barcode reads.\nSmall package logistics, like the example shown here, is an ideal environment for AI learning, with constantly changing packages and scenes at every time step that make it perfectly suited for neural networks.\nThese improvements were achieved through both data scaling and model architectural improvements:\nTemporal memory – A new vision memory module gives Helix stateful perception. The policy now also incorporates a history of past states, enabling temporally extended behaviors and improved robustness to interruptions.\nForce feedback – Force sensing is integrated into the state input, providing a sense of touch proxy that leads to more precise grips and manipulation of packages.\nHere we analyze the sources of these gains, examining how increasing the demonstration training data (from 10 to 60 hours) affects performance, and how each of the above architectural enhancements contributes to Helix’s speed and accuracy in package handling.\nExpanded Package Variety and Adaptive Behaviors\nHelix’s logistics policy has broadened to handle a much wider diversity of packages. In addition to standard rigid boxes, the system now manages polyethylene bags (poly bags), padded envelopes, and other deformable or thin parcels that pose unique challenges. These items can fold, crumple, or flex, making it harder to grasp and locate labels. Helix addresses this by adjusting its grasp strategy on the fly – for example, flicking away a soft bag to flip it dynamically, or using pinch grips for flat mailers. Despite the greater variety in shape and texture, Helix increased its throughput, processing items in about 4.05 seconds each on average without bottlenecks.\nThe goal of this logistics task is to rotate the package so the barcode faces downward for scanning. One notable behavior is Helix’s tendency to pat down plastic packaging before attempting to scan it. If a shipping label lies on a curved or wrinkled surface (common with loosely filled poly bags or bubbled envelopes), the policy reacts by briefly pressing and smoothing the surface flat. This subtle “flattening” action, learned from demonstrations, ensures the barcode is fully visible to the scanner. Such adaptive behavior highlights the advantage of end-to-end learning – the robot learns from demonstration strategies that were never explicitly hard-coded, directly from the data, to overcome real-world imperfections in packaging.\nCrucially, these new capabilities did not compromise efficiency. Throughput has increased alongside versatility. Helix’s average handling time per package dropped from roughly 5.0 seconds on a simplified set of packages to 4.31 seconds, even as the task got harder with new package types. This speed-up brings performance closer to human operator speeds. Likewise, barcode orientation success climbed to ~95%. Together, these improvements indicate a more dexterous and reliable system, one that can approach human-level speed and accuracy across a broad spectrum of real-world parcels.\nArchitectural Enhancements to Helix’s Visuo-Motor Policy\nMany of the above gains were enabled by targeted improvements to Helix’s System 1 visuo-motor policy. Over the past two months, we introduced new modules for memory and sensing that make the control policy more context-aware and robust. These enhancements allow Helix to better perceive the state of the world over time and feel what it’s doing, complementing the vision and control foundation established in the initial deployment. Here we detail each improvement and how it contributes to Helix’s logistics performance.\nVision Memory\nHelix’s policy now maintains a short-term visual memory of its environment, rather than acting only on instantaneous camera frames. Concretely, the model is equipped with a module that composes features from a s",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 81307,
    "processed_at": "2025-09-10T11:41:35.374601"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/news/botq",
    "key_url": "https://figure.ai/news/botq",
    "page_type": "news",
    "content_hash": "7e24f0941412ab7fc2880ed31abc2c0f9653bc4621be30ed562f09d41d5903b4",
    "normalized_text_len": 8587,
    "extracted_text_preview": "Introducing BotQ\nIntroducing BotQ, Figure’s new high-volume manufacturing facility for humanoid robots. BotQ’s first generation manufacturing line will be capable of manufacturing up to 12,000 humanoids per year. This is just the start and we will be scaling up considerably from here. Over the past eight months, the Figure hardware and manufacturing engineering teams have been building an efficient and scalable manufacturing process from the ground up, and we’re excited to share our progress to date.\nVertically Integrated Manufacturing: Figure has made the decision to bring the manufacturing of our humanoid robots in-house to control the build process, quality, and guarantee that we will deliver a high-functioning robot to market.\nBuilding Software Infrastructure: For the last six months, we’ve been building the underlying software infrastructure that can support volume manufacturing (MES, PLM, ERP, WMS).\nRobots Building Robots: Figure’s humanoid robots will be used in our manufacturing process to build other humanoid robots. This will take place this year. We project that the number of our humanoid robots involved will grow substantially over time to increase line automation.\nOur Roadmap To Scalable Manufacturing\nRethinking the robot architecture\nAchieving high production rate starts at the early stage of engineering design. After building and internally manufacturing Figure 02, the team recorded and analyzed cycle times for every process, from part fabrication to final assembly. The largest driver of assembly time started at the root: part count and manufacturing processes. Figure 02 was designed as a prototype and extensively used high complexity, tight tolerance, slow computer numerical control (CNC) machining processes. While CNC from billet is a great process for prototyping and holds value for high tolerance components, it does not scale well when trying to drastically reduce both part costs and time to make components. Part count was also heavily considered - if you have to join two parts, it takes time to perform that operation. Instead, if you can combine many parts into one, the cycle time goes down.\nWe have since completed the design of our next-generation robot, Figure 03, which is our production robot built for affordability and high-volume manufacturing. In order to achieve production manufacturing rates, we switched to tooled processes such as injection molding, diecasting, metal injection molding and stamping which enabled us to save thousands of hours on manufacturing. Parts that previously spent over a week on a CNC machine can now be manufactured in under 20 seconds with complex steel molds. Switching to these processes comes with a high capital cost, but the upfront investment is made back quickly when considering our robot volumes for 2025 and beyond. Rethinking the robot architecture also led us to stand up two new teams - a safety team and a reliability team.\nIn addition to manufacturing at high rates, we are also ramping up our focus on robot reliability. The reliability team, located at BotQ, is in charge of running highly accelerated lifecycle tests to help us understand the lifespan of the robot. The reliability team needs specialized equipment, including high temperature ovens, dedicated actuator testers, and failure analysis equipment to understand root cause failures. Using this data, we can inform the robot designers of improvements needed to ensure our reliability metrics are met.\nBuilding the supply chain\nHumanoid robots, unlike most other industries, do not have well established supply chains with various tiers of manufacturers building modules of the system. This has ultimately led us to design almost the entire robot from scratch including the actuators, motors, sensors, battery pack, and electronics. The lack of a mature supply chain has been one of the biggest hurdles our team has faced when defining how parts would be made and where. What would we vertically integrate, what would we choose to tackle in house? We made the decision to focus heavily on the assembly of our core technology in-house (actuators, hands, batteries, final assembly) and when needed, leverage outside vendors for piece part manufacturing. In addition, we hired a team of world-class global supply managers to help form strong partnerships with vendors who have the expertise to produce our complex components. Our robot has over 3 dozen unique commodities, many of which require unique processes such as motor winding, flexible OLED screens, or precise optical designs. All of our external partners were selected to be able to scale with us: our supply chain can easily scale to 100,000 robots or 3,000,000 actuators in the next four years.\nBuilding the manufacturing team\nThe next organization that we needed to develop was the manufacturing team. Over the last six months, we’ve hired experts who spent their careers designing lines, optimizing those lines and understanding how to optimize the process of turn",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 52408,
    "processed_at": "2025-09-10T11:41:35.348549"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/news/helix-logistics",
    "key_url": "https://figure.ai/news/helix-logistics",
    "page_type": "news",
    "content_hash": "9088353f4d122325aa1c4976d12bb21d0bbda127a5bfb14c3efb12d3d9ed2ec0",
    "normalized_text_len": 8206,
    "extracted_text_preview": "Bringing humanoid robots into the workforce is at the heart of Figure’s mission. Today, we’re introducing a new real-world application for Figure robots: logistics package manipulation and triaging. This task demands human-level speed, precision, and adaptability, pushing the boundaries of pixels-to-actions learned manipulation. Last week we introduced Helix, Figure’s internally designed Vision-Language-Action (VLA) model that unifies perception, language understanding, and learned control. In this report, we focus on a series of general improvements we made to System 1 (S1) of Helix, the low-level visuo-motor control policy, while iterating on this challenging new commercial use case:\nImplicit stereo vision – Helix System 1 now has rich 3D understanding enabling more precise depth-aware motion.\nMulti-scale visual representation – The lower-level policy captures fine-grained details while retaining scene level understanding for more accurate manipulation.\nLearned visual proprioception – Each Figure robot can now calibrate itself, making cross-robot transfer seamless.\nSport mode – Using a simple test-time speed up technique, Helix achieves faster-than-demonstrator execution speed while maintaining high success rate and dexterity.\nWe also explore the trade off between data quality and quantity for this particular use case, and show that just 8 hours of well curated demonstration data can yield a dexterous and flexible policy.\nThe Use Case\nPackage handling and sorting is a fundamental operation in logistics. This often involves transferring packages from one conveyor belt to another, while also ensuring the shipping label is correctly oriented for scanning. This task presents several key challenges: packages may come in a wide variety of sizes, shapes, weights, and rigidity – from rigid boxes to deformable bags, making it difficult to replicate in simulation. The system must determine the optimal moment and method for grasping the moving object and reorienting each package to expose the label. Furthermore, it needs to track the dynamic flow of numerous packages on a continuously moving conveyor and maintain a high throughput. As the environment can never be fully predictable, the system must be able to self-correct. Addressing these challenges isn't only a key application of Figure's business, it also yielded generic new improvements to Helix System 1 that all other use cases now benefit from.\nArchitectural Improvements to Helix's Visuo-Motor Policy (System 1)\nVisual representation\nWhere our prior System 1 relied on monocular visual input, our new System 1 now leverages a stereo vision backbone coupled with a multiscale feature extraction network to capture rich spatial hierarchies. Rather than feeding image feature tokens from each camera independently, features from both cameras are merged in a multiscale stereo network before being tokenized, keeping the overall number of visual tokens fed to our cross-attention transformer constant and avoiding computational overhead. The multiscale features allow the system to interpret fine details as well as broader contextual cues, together contributing to more reliable control from vision.\nCross robot transfer\nDeploying a single policy on many robots requires addressing distribution shifts in the observation and action spaces due to small individual robot hardware variations. These include sensor calibration differences (affecting input observations) and joint response characteristics (affecting action execution), which can impact policy performance if not properly compensated for. Especially with a high dimensional whole-upper-body action space, traditional manual robot calibration doesn't scale over a fleet of robots. Instead, we train a visual proprioception model to estimate the 6D poses of end effectors entirely from each robot's onboard visual input. This online \"self-calibration\" allows strong cross-robot policy transfer with minimal downtime.\nData curation\nOn the data side, we took particular care in filtering human demonstrations, excluding the slower, missed, or failed ones. However, we deliberately kept demonstrations that naturally included corrective behavior, when the failure that prompted the correction was deemed due to environmental stochasticity rather than operator error. Working closely with the teleoperators to refine and uniformize manipulation strategies also resulted in significant improvements.\nInference-time manipulation speedup\nOur systems need to approach and eventually go beyond human manipulation speed. We apply a simple, but effective, test-time technique that yields faster-than-demonstrator learned behavior: interpolate the policy action chunk output (we call this “Sport Mode”). Our S1 policies output action \"chunks\", representing a series of robot actions at 200hz. In practice, we can for example achieve a 20% test-time speedup, without any modifications to the training procedure, by linearly re-sampling an action chunk of [T x action_",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 91908,
    "processed_at": "2025-09-10T11:41:36.117876"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/news/f-03-battery-development",
    "key_url": "https://figure.ai/news/f-03-battery-development",
    "page_type": "product",
    "content_hash": "35919972663aee6665c53752f90e829868d4aa9c84472f04260ff281ed423643",
    "normalized_text_len": 7562,
    "extracted_text_preview": "Introducing F.03 Battery\nToday, we are presenting the Figure 03 (F.03) battery — a significant advancement in our core humanoid robot technology roadmap. With battery technology at the heart of our electromechanical humanoid platform, we decided to engineer and manufacture the battery system in-house at BotQ. Starting from F.01, we have built custom battery packs for our application. F.01 used bulky rectangular modules and components that only fit in an external backpack. Through advancements across three generations of battery technology, we have increased the energy density by 94%, integrated the battery directly into the torso, and dramatically improved safety and abuse tolerance.\nThe F.03 battery pushes the bounds across all key battery attributes:\nRun Time: 2.3 kWh enables 5 hours of run time at peak performance\nMass & Volume Efficiency: Significant energy density improvements packaged into our most compact robot to date\nFast Charge: 2 kW fast charge with active cooling\nBattery Management System (BMS): Custom BMS to maintain battery health, optimize performance, and prevent fault conditions\nSafety: Innovative multi-layer safety architecture targeting both UN & UL safety certification\nReliability: Survives rigorous environmental, mechanical and electrical test suite\nCost: 78% reduction in cost over F.02\nScale: Mass production components and final assembly processes\nThe Optimal Robot Battery Architecture\nFor the F.03 battery, we took on the challenge of dramatically increasing gravimetric and volumetric energy density while raising the bar for safety and reliability. In order to accomplish this, we had to think carefully about every design decision and rapidly analyze, design, test and iterate. The F.03 battery employs a number of novel design strategies including:\nMulti-Function Components: Multiple functions were integrated into nearly every part, resulting in a high cell-to-pack ratio, lower cost, and lower complexity.\nStructural Battery: The battery enclosure is constructed of high strength stamped steel, die cast aluminum and structural adhesives. This enables the battery to survive a variety of demanding structural requirements, such as being dropped 1 meter onto concrete from any orientation. Furthermore, this enables the battery to serve as a structural member of the robot torso, which saves mass and volume at the robot level.\nActive Cooling System: Cooling components are directly integrated into the die casting. By minimizing thermal resistances and local heat generation, fast charge can be achieved with simple forced convection cooling.\nAnti-Propagation & Flame Containment System: In the event that one cell goes into thermal runaway from abuse conditions, the pack features a number of safety measures to prevent the thermal runaway from propagating to adjacent cells and to prevent flame from exiting the pack. Anti-propagation is achieved by using a thermally insulative potting compound in concert with a rapid heat distribution strategy. Flame containment is accomplished with a multi-function flame arrestor pack vent and a patented technology to prevent an external flame from exiting the pack.\nRaising the Bar for Robot Battery Safety\nAt Figure, achieving a high level of safety is a non-negotiable. The F.03 battery pushes the bounds for robot battery safety by implementing multiple lines of defense against battery abuse or malfunction. The layers of safety include:\nBattery Management System (BMS) Protections: The custom BMS includes an array of sensors, switches and fuses to prevent fault conditions such as overcharge, overdischarge, over temperature and external short circuit.\nCell Protections: The cell itself has been certified to numerous UN, UL and IEC safety standards ensuring tolerance for abuse conditions such as crush, impact, overcharge, heating, etc. The cell also includes 2 mechanisms of internal fusing in the event of short circuit.\nInterconnect Protection: The cell-to-cell wirebond interconnect geometry is tuned to act as a fusible element to serve as an additional layer of short circuit protection.\nPack Protections: In the event of severe abuse conditions, the anti-propagation and flame quench system can contain a thermal runaway event and ensure a safe outcome.\nAs part of Figure’s commitment to differentiate our humanoid as the leader in robot safety, we specified that the battery system of F.03 must not emit flames should a catastrophic failure of a single cell occur. This critical decision underscores our deep commitment to product safety. One sample of the fault injection test result is shown below. We intentionally induced failure by heating a cell to the point of thermal runaway. The F.03 battery is able to prevent external flame and attenuate cell-to-cell thermal propagation. The team achieved this goal by developing a number of proprietary technologies over the course of a year using thermodynamic modeling, characterization, methodical physical testing and iteration.\nThe F.03",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 61662,
    "processed_at": "2025-09-10T11:41:35.975744"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/master-plan",
    "key_url": "https://figure.ai/master-plan",
    "page_type": "pricing",
    "content_hash": "e91477b60142692091b0b8a289e10ff1fba2127d13cc429af7e9066a8bfdd9fd",
    "normalized_text_len": 7527,
    "extracted_text_preview": "Roadmap to a positive future powered by AI\nBackground: I’m 20 years into building technology companies, previously the Founder of Archer ($2.7B IPO) and Vettery ($100M exit). My sole focus is Figure. My ambition is to build this company with a 30-year view, spending my time and resources on maximizing my utility impact to humanity.\nAbout UsOur Mission\nExpand human capabilities through advanced AI.\nThe Company\nI believe that positively affecting the future of humanity is the moral priority of our time. The most meaningful impact can come from dedicating our resources to developing technologies. In the coming age we will see great advancements in Artificial Intelligence (AI) and Robotics, and by contributing in the early stages, we can set the course for a positive AI future for humanity.\nHence the goal of Figure: to develop general purpose humanoids that make a positive impact on humanity and create a better life for future generations. These robots can eliminate the need for unsafe and undesirable jobs — ultimately allowing us to live happier, more purposeful lives.\nOur company journey will take decades — and require a championship team dedicated to the mission, billions of dollars invested, and engineering innovation in order to achieve a mass-market impact. We face high risk and extremely low chances of success. However, if we are successful, we have the potential to positively impact humanity and to build the largest company on the planet.\nThe Present\nToday, we are seeing unprecedented labor shortages. There are over 10 million unsafe or undesirable jobs in the U.S. alone, and an aging population will only make it increasingly difficult for companies to scale their workforces. As a result, the labor supply growth is set to flatline this century. If we want continued growth, we need more productivity — and this means more automation.\nThe Possibility\nThankfully, we are in the early stages of an AI and Robotics revolution. This presents the unique opportunity to substantially increase our production and standard of living.\nAs automation continues to integrate with human life at scale, we can predict that the labor-based economy as we know it will transform. Robots that can think, learn, reason, and interact with their environments will eventually be capable of performing tasks better than humans. Today, manual labor compensation is the primary driver of goods and services prices, accounting for ~50% of global GDP (~$42 trillion/yr), but as these robots “join the workforce,” everywhere from factories to farmland, the cost of labor will decrease until it becomes equivalent to the price of renting a robot, facilitating a long-term, holistic reduction in costs. Over time, humans could leave the loop altogether as robots become capable of building other robots — driving prices down even more. This will change our productivity in exciting ways. Manual labor could become optional and higher production could bring an abundance of affordable goods and services, creating the potential for more wealth for everyone.\nWe will have the chance to create a future with a significantly higher standard of living, where people can pursue the lives they want.\nWe believe humanoids will revolutionize a variety of industries, from corporate labor roles (3+ billion humans), to assisting individuals in the home (2+ billion), to caring for the elderly (~1 billion), and to building new worlds on other planets. However, our first applications will be in industries such as manufacturing, shipping and logistics, warehousing, and retail, where labor shortages are the most severe. In early development, the tasks humanoids complete will be structured and repetitive, but over time, and with advancements in robot learning and software, humanoids will expand in capability and be able to tackle more complex job functions. We will not place humanoids in military or defense applications, nor any roles that require inflicting harm on humans. Our focus is on providing resources for jobs that humans don’t want to perform.\nWe see three major business opportunities in the long term\nMore Structured Less Variability\nLess Structured More Variability\nPhysical Labor\n- 50% of global GDP is human labor ($42T)\nConsumer Household\n- 2.3 billion households worldwide\n- 700M aging population in need of at-home care\nOff-World\n- Space exploration to build new worlds\nLess Structured More Variability\nThe Solution\nThere are two schools of thought on how to solve real-world robotics: build an environment specifically for robots, or reverse it and build robots for our human environment. We could have either millions of different types of robots serving unique tasks or one humanoid robot with a general interface, serving millions of tasks. At Figure, we believe general purpose humanoid robots built for a human environment is the desired route to have the largest overall impact. For that reason, our humanoid robots resemble the human body in shape — two legs, two arms, hand",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 61531,
    "processed_at": "2025-09-10T11:41:36.144178"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/news/reinforcement-learning-walking",
    "key_url": "https://figure.ai/news/reinforcement-learning-walking",
    "page_type": "other",
    "content_hash": "629af3f18a11f837209bc5e05ec2fd55ac5bcbfec0375a4dc98cd64d845aab3b",
    "normalized_text_len": 5018,
    "extracted_text_preview": "Introducing Learned Natural Walking\nWe are excited to introduce our end-to-end neural network, trained with reinforcement learning (RL), for humanoid locomotion.\nLeveraging Reinforcement Learning: RL uses trial-and-error in simulation to teach Figure 02 humanoid robot how to walk like a human.\nTrained in Simulation: Our robot learns to walk similar to a human via a high fidelity physics simulator. We simulate years of data in only a few hours.\nSim-to-Real Transfer: By combining domain randomization in simulation with high-frequency torque feedback on the robot, policies trained in sim transfer zero-shot to real hardware without additional tuning.\nOur Approach\nReinforcement Learning (RL) is an AI approach where a controller learns through trial and error, optimizing behaviors based on a reward signal.\nFigure trained our RL controller in high-fidelity simulations, running thousands of virtual humanoids with varied parameters and scenarios. This diverse exposure allows our trained policy to transfer directly (“zero-shot”) from simulation to Figure 02 robots, providing robust and human-like walking. Figure’s RL-driven training shortens development cycles and consistently delivers robust real-world performance.\nBelow we will dive into engineering our robots to walk like humans, the training process in simulation, and how we zero-shot to the real robot.\nReinforcement Learning Training\nWe trained our new walking controller fully in a GPU accelerated physics simulation using reinforcement learning, collecting years worth of simulated demonstrations in a few hours.\nIn our simulator, thousands of Figure 02 robots are simulated in parallel, each with unique physical parameters. These robots are then exposed to a wide range of scenarios they might encounter, and a single neural network policy learns to operate them all. This includes encountering various terrains, changes in actuator dynamics, and responses to trips, slips, and shoves.\nEngineering Robots That Walk Like Humans\nThe benefit of a humanoid robot is one general hardware platform that can do human-like applications. And over time, we want our robot to move more like a human through the world.\nA policy learned using RL might converge to sub-optimal control strategies that do not capture the stylistic attributes that define human walking. This includes walking with a human-like gait, with heel-strikes, toe-offs and arm-swing synchronized with leg movement. We inject this preference into our learning framework by rewarding the robot to mimic human walking reference trajectories. These trajectories establish a prior over the walking styles the policy is allowed to generate, while additional reward terms optimize for velocity tracking, power consumption and robustness to external perturbations and variations in terrain.\nSim-to-Real Transfer\nThe final step is getting the policy out of simulation and into a real humanoid robot. A simulated robot is, at best, only an approximation of a high-dimensional electro-mechanical system, and a policy trained in simulation is guaranteed to work only on these simulated robots.\nTo bridge this “sim-to-real gap” we use a combination of domain randomization in simulation and a kHz-rate torque feedback control on the robot. Domain randomization bridges the sim-to-real gap by randomizing the physical properties of each robot, simulating a breadth of systems the policy may have to run on. This helps the policy to generalize zero-shot to a physical robot without any additional fine-tuning.\nWe additionally run the policy output through kHz-rate closed-loop torque control to compensate for errors in actuator modeling. The policy is robust to robot-to-robot variations, changes in surface friction and external pushes, producing repeatable human-like walking across the entire fleet of Figure 02 robots. This is highly encouraging, as it indicates our technology can scale effectively across the entire fleet, without any additional engineering effort, supporting broader commercial operations.\nHere you can see 10 Figure 02 robots that are all operating on the same RL neural network with no tweaks or changes. This gives us hope this process can scale to thousands of Figure robots in the near future.\nConclusion\nWe have presented a natural walking controller learned purely in simulation using end-to-end reinforcement learning. This enables the fleet of Figure robots to quickly learn robust, proprioceptive locomotion strategies and enables rapid engineering iteration cycles.\nThese initial results are exciting, but we believe they only hint at the full potential of our technology. We’re committed to extending our learned policy to handle every human-like scenario the robot might face in the real world. If you’re intrigued by the possibilities of scaling reinforcement learning and the future of dexterous humanoid robotics, we invite you to join us on this journey.\nConsider joining our team to help scale Embodied AI to millions of robots. Check out ou",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 43381,
    "processed_at": "2025-09-10T11:41:36.066666"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/news/helix-learns-to-fold-laundry",
    "key_url": "https://figure.ai/news/helix-learns-to-fold-laundry",
    "page_type": "news",
    "content_hash": "4ed5e81547420b1658869834b5fa83b9972ab0314fcf0be18f0cf09fbea7d846",
    "normalized_text_len": 2432,
    "extracted_text_preview": "Helix, Figure’s Vision Language Action (VLA) model, recently demonstrated an hour of fully autonomous package reorientation in a logistics setting. Now, the same model is tackling something entirely different: folding laundry.\nFolding laundry sounds mundane for a person, but this is one of the most challenging dexterous manipulation tasks for a humanoid robot. Towels are deformable, constantly changing shape, bending unpredictably, and prone to wrinkling or tangling. There’s no fixed geometry to memorize, and no single “correct” grasp point. Even a slight slip of a finger can cause the material to bunch or fall. Success requires more than just seeing the world accurately - it demands fine, coordinated finger control to trace edges, pinch corners, smooth surfaces, and adapt in real time.\nKey Results:\nA first for humanoids. This is the first instance of a humanoid robot with multi-fingered hands folding laundry fully autonomously using an end-to-end neural network.\nSame architecture, data-only change. The same Helix architecture that solved logistic tasks was applied directly to laundry folding - with no modifications to the model or training hyperparameters. The only addition was the dataset.\nNatural multimodal interaction. In addition to folding, Helix learned to maintain eye contact, direct its gaze, and use learned hand gestures while engaging with people.\nWithout any architectural changes, Helix learned to:\nPick towels from a mixed pile.\nAdjust folding strategies based on starting configurations.\nRecover from multi-pick errors by returning extra items.\nUse fine manipulation skills, like tracing an edge with a thumb, pinching corners, or unraveling tangled towels - before completing folds.\nCritically, Helix does all of this without explicit object-level representations. For highly deformable items like towels, building such representations is brittle and unreliable. Instead, Helix operates entirely end-to-end: from vision and language input to smooth, precise motor control.\nWhy this matters\nThe same general-purpose architecture, and the same physical platform, can seamlessly transition from industrial logistics to household chores. As we scale real-world data collection, we expect Helix’s dexterity, speed, and generalization to keep improving across an even broader range of tasks.\nIf you’re interested in helping us push the frontier of general-purpose humanoid intelligence, we’re hiring.",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 35830,
    "processed_at": "2025-09-10T11:41:36.086281"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/news/helix-loads-the-dishwasher",
    "key_url": "https://figure.ai/news/helix-loads-the-dishwasher",
    "page_type": "news",
    "content_hash": "40b8de790eb2f3f8986179972c81e998ebdb8885e122473481f357cacd176473",
    "normalized_text_len": 1871,
    "extracted_text_preview": "Helix, Figure’s Vision Language Action (VLA) model, has shown it can adapt to dramatically different real-world challenges with nothing more than new data. After folding laundry and rearranging packages, Helix is now taking on another everyday task: loading a dishwasher.\nAt first glance, loading a dishwasher sounds like a simple task - just pick up each object and place it in the dishwasher. But in reality, dishwasher loading bundles together many difficult problems in robotics: dishes often need to be isolated from cluttered stacks, reoriented, or handed off between two arms working in sync; slippery or fragile items demand fingertip-level precision; and dishwasher racks provide only centimeter-scale tolerance for error. On top of that, every load is different - novel objects, messy starting states, and unexpected collisions mean the system must constantly adapt and recover while maintaining robust performance.\nKey Results\nThe same Helix model that folded towels and sorted packages can now load a dishwasher. No new algorithms, no special-case engineering, just new data.\nWith that, Helix learned to:\nSingulate stacked plates and load them in order.\nPick up a glass with one hand, reorient it, and place it carefully with the other.\nAdjust its strategy to account for messy starting configurations.\nRecover gracefully from errors like misgrasps or collisions.\nWhy This Matters\nDishwasher loading, package logistics, and towel folding may seem worlds apart, yet Helix handles all with the same general-purpose architecture. This represents another step toward scalable humanoid intelligence: a single system that can incrementally learn new capabilities with additional data, building toward broader applicability without special-case engineering.\nJoin us on our mission to bring general purpose learning humanoid robots into the home and global workforce.",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 34077,
    "processed_at": "2025-09-10T11:41:35.386172"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/about-us",
    "key_url": "https://figure.ai/about-us",
    "page_type": "company",
    "content_hash": "58436003f3fbf951371989b5f6bc04eff63e4a0b5838cf66e093b56d9b5ad4e1",
    "normalized_text_len": 1550,
    "extracted_text_preview": "DESIGNING ROBOTS FOR THE REAL WORLD\nFIGURE IS ALWAYS EVOLVING.\nWe’ve completed two generations of robots in three years. Join us to build Figure 03.\nOUR VALUES\nMove Fast & Be Technically Fearless\nHesitation is the enemy of momentum. We are tackling today’s most complex technological challenges by testing, experimenting, and taking calculated risks to embrace the unknown without fear of failure.\nProduct First, Mission Focused\nOur product and our mission are one in the same: to bring a commercially viable humanoid to market. We are builders, designers, and engineers united by a commitment to that mission. We avoid distraction and unrelated activities to remain laser-focused on shipping a safe, high quality product.\nAggressively Optimistic\nBuilding Figure won’t be an easy win; it will require decades of commitment and ingenuity. We’re humbled by our mission, and aspire to remain optimistic even in the face of enormous hurdles.\nMaximize Future Impact\nWe have what it takes to build the most groundbreaking company on the planet — to create an inspiring future for generations to come with improved access to goods and services, safer working conditions, and more opportunity for fulfilling work. Our focus is on what we can achieve 5, 10, 20+ years from now, not the near-term wins.\nChampionship Mindset\nTo address the extraordinary demands of our work, we believe in operating as a winning team. We are in the trenches together, collaborating in-person, remaining hyper-focused, and pushing each other to the highest levels of performance.",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 52253,
    "processed_at": "2025-09-10T11:41:35.515569"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/",
    "key_url": "https://figure.ai/",
    "page_type": "product",
    "content_hash": "f6c955cbaa918a81731f10d10d12a79aa0c944ce8f7a5f5f91c5d8494cbd476e",
    "normalized_text_len": 1361,
    "extracted_text_preview": "Figure is the first-of-its-kind AI robotics company bringing a general purpose humanoid to life.\nFIGURE IS FINALLY BRINGING ROBOTS HOME.\nFIGURE IS PUTTING LEARNING ROBOTS INTO THE GLOBAL WORKFORCE.\nFIGURE IS TAKING AN AI-FIRST APPROACH TO MARKET.\nWe’re engineering the first humanoid that can truly think for itself.\nFigure is poised to solve labor shortages. Up and running for our first commercial customers, it’s learning and performing end-to-end operations safely. Now, in the biggest innovation since the dishwasher, Figure is finally coming home. Expect Figure to do dishes and laundry, unpack groceries, and perform other household tasks 100% autonomously.\nWE’RE GIVING AI A BODY.\nWhy a human form factor?\nWe’ve designed our world for the human form. Hands allow us to open doors and use tools; arms and legs allow us to move efficiently, climb stairs, lift boxes, and more. Figure brings together the dexterity of the human form and cutting edge AI to go beyond single-function robots, lending support at home and across manufacturing, logistics, warehousing, and retail.\nSEE FIGURE IN ACTION.\n- Height\n- 5'6\"\n- Payload\n- 20kg\n- Weight\n- 70kg\n- Runtime\n- 5hr\n- Speed\n- 1.2M/S\n- System\n- Electric\nThe team bringing impossible ideas to life.\nFigure has attracted the world’s leading robotics team with over 100 years of combined AI & Humanoid experience.",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 152580,
    "processed_at": "2025-09-10T11:41:35.320136"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/news",
    "key_url": "https://figure.ai/news",
    "page_type": "product",
    "content_hash": "81b74475bee8acbbd19e55cf39b1186b0b5838620e8362e8c0af9a609562d0e9",
    "normalized_text_len": 384,
    "extracted_text_preview": "Highlights\nAll News\nHelix Loads the Dishwasher\nHelix Learns to Fold Laundry\nF.03 Battery Development\nScaling Helix: a New State of the Art in Humanoid Logistics\nNatural Humanoid Walk Using Reinforcement Learning\nBotQ: A High-Volume Manufacturing Facility for Humanoid Robots\nHelix Accelerating Real-World Logistics\nHelix: A Vision-Language-Action Model for Generalist Humanoid Control",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 38589,
    "processed_at": "2025-09-10T11:41:35.531382"
  },
  {
    "fingerprint_session_id": 9,
    "url": "https://figure.ai/ai",
    "key_url": "https://figure.ai/ai",
    "page_type": "product",
    "content_hash": "5b12fa4ab64656820700c7187d86e6b01e3aca4381457a15eba2252b445418d5",
    "normalized_text_len": 375,
    "extracted_text_preview": "Helix is a generalist humanoid Vision-Language-Action model that reasons like a human\nWE BELIEVE ARTIFICIAL INTELLIGENCE IS THE KEY TO GENERAL PURPOSE ROBOTICS.\nWE’RE BUILDING ROBOTS THAT LEARN AND REASON LIKE HUMANS.\nOUR ROBOTS WILL ACQUIRE NEW SKILLS AND IMPROVE OVER TIME.\nRead our latest technical writeups on Helix’s AI system and its real-world applications.\nThe latest",
    "low_text_pdf": false,
    "needs_render": false,
    "fetch_status": 200,
    "content_type": "text/html",
    "content_length": 144603,
    "processed_at": "2025-09-10T11:41:35.469375"
  }
]